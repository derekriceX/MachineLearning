---
title: "KNN"
author: "Rice"
date: "2023-07-16"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.



## ML for R textbook
## chapter 3 KNN
## working examples in the book

```{r}
print("hello machine learning")
print("hello from GitHub")
```

```{r}
# transformation - normalizing numeric data 

normalize <- function (x){
  return ((x-min(x))/(max(x)-min(x)))
  }

```


```{r}
setwd("D:/Dropbox/13 - R-WD/MachineLearning/MachineLearning")
wbcd <- read.csv("wisc_bc_data.csv",header=TRUE)
head(wbcd)
str(wbcd)
```




```{r}
wbcd <- wbcd[-1]        # remove 1st column from the dataset 
table(wbcd$diagnosis)   # diagnosis is the target outcome B=357 vs M=212 instances 
str(wbcd$diagnosis)     # currently a chr - need target to be a factor 
wbcd$diagnosis <- factor(wbcd$diagnosis, levels =c("B","M"),labels = c("Benign","Malignant"))
str(wbcd$diagnosis)     # Factor w/ 2 levels "Benign","Malignant": 1 1 1 1 1 1 1 2 1 1 ...
prop.table(table(wbcd$diagnosis)) #returns decimal fraction to eight decimal places
round(prop.table(table(wbcd$diagnosis))*100,digits=1) # Benighn = 62.7 and Malignant = 37.3
summary(wbcd) # 30 features all numeric three different measurements of 10 char 

# all 30 features are on different scales

normalize(c(1,2,3,4,5)) # function seems to be working 
normalize(c(10,20,30,40,50)) # get the same normalized vaues 

# the lapply() function takes a list and applies a specified function to each list element 
# a data frame is a list of equal length vectors, we can use lapply() to apply normalize() to exach feature in the data frame 
# the final step is to convert the list return by lapply() to a data frame using the as.data.frame() function 

wbcd_n <- as.data.frame(lapply(wbcd[2:31],normalize))
head(wbcd_n)

```
```{r}
summary(wbcd_n)
```


```{r}
## step 2
## data preparation - creating training and test datasets 

wbcd_train <- wbcd_n[1:469,]
wbcd_test <- wbcd_n[470:569,]
wbcd_train_labels <- wbcd[1:469,1]
wbcd_test_labels <- wbcd[470:569,1]
```

Step 3 - training a KNN model on the data 
# uses library(class) - provides a basic R functions for classification
# The knn() function in the class package provides a standard, traditional implementation of the k-NN algorithm
# for each instance in the test data, the function will identify the k nearest neighbors using elclidean distance where k is a user-specifed number 

# kNN classification syntex
# using the knn() function in the class package
# building the classifer and making predictions:
## p <- knn(train,test,class,k)
### train - is a data frame containing numberic training data
### test - is a data frame containing numeric test data
### class is a factor vector with class for each row in the training data
### k is an integer indicating the number of nearest neighbors 
# the function returns a factor vector of predicted classes for each row in the test data frame 


```{r}
# Step 3 - training a kNN model
library(class)
wbcd_test_pred <- knn(train = wbcd_train,test=wbcd_test,cl=wbcd_train_labels,k=12)

```

Step 4 - evaluating model performance 

uses library(gmodels) includes the CrossTable() function - eval how well the predicted classes wbcd_test_pred vectot match the actual values wbcd_test_labels vector 
````{r}
library(gmodels)
CrossTable(x=wbcd_test_labels,y=wbcd_test_pred,prop.chisq=FALSE) # FALSE excludes the unnecessary chi-squared values from the output


```

### Step 5

# we will attempt two simple variation on our previous classifier 
# 1st - employ an alternative method for rescalling our numeric features
# 2nd - try several different k values 
# transformation - z-scope standardization
# z-score have no predefined min or max - extreme values are not compressed toward the center 
# tumors might be correlated with extreme outliers 
# R's built in scale() function by default rescales values using the z-score std 
# the scale() function can be applied directly to a data frame so no need to use lapply() function

# mean of z-score is always 0 - extreme values are outside +/-3z

```{r}
# Step 5 - improving the model 
# to creat a z-score std version of the wbcd data use the following

wbcd_z <- as.data.frame(scale(wbcd[-1]))
print("mean of z-score is always 0 - extreme values are outside +/-3z")
cat("\n")
cat("\n")

summary(wbcd_z)
```

```{r}
## create train and test data - run model - evaluate model 

wbcd_train <- wbcd_z[1:469,]
wbcd_test <- wbcd_z[470:569,]
wbcd_train_labels <- wbcd[1:469,1]
wbcd_test_labels <- wbcd[470:569,1]

wbcd_test_pred <- knn(train = wbcd_train, test=wbcd_test,cl=wbcd_train_labels,k=21)

CrossTable(x=wbcd_test_labels,y=wbcd_test_pred,prop.chisq=TRUE)

```
```{r}
### running a for loop to determine best value for k 

k_values <- c(1,5,11,15,21,27)
for (k_val in k_values){
  wbcd_test_pred <- knn(train=wbcd_train,
                        test = wbcd_test,
                        cl=wbcd_train_labels,
                        k=k_val)
  CrossTable(x=wbcd_test_labels,
             y=wbcd_test_pred,
             prop.chisq=FALSE)
}
```
```{r}
# install.packages("car")
#library(ggplot2)
#library(car)
```

```{r}
## create a regression model for Error Rate as a function of K 

k = c(1,5,11,15,21,27)
y = c(4,2,3,3,2,4)
model <- lm(y~poly(x,degree=2,raw=TRUE))

# Generate predicted values

x_pred <- seq(min(k), max(k), length.out = 100)
y_pred <- predict(model, newdata = data.frame(x = x_pred))

# Create scatter plot of the data
plot(x, y, pch = 16, main = "Quadratic Regression", xlab = "k", ylab = "er")

# Add the quadratic regression line
lines(x_pred, y_pred, col = "red", lwd = 2)
```



```{r}
# Generate example data
# code from ChatGPT to help setup the regression model 

#x <- seq(-10, 10, by = 0.1)
#y <- 2*x^2 + 3*x - 5 + rnorm(length(x), 0, 10)

x = c(1,5,11,15,21,27)
y = c(4,2,3,3,2,4)

# Fit quadratic regression model
model <- lm(y ~ poly(x, degree = 2, raw = TRUE))

# Generate predicted values
x_pred <- seq(min(x), max(x), length.out = 100)
y_pred <- predict(model, newdata = data.frame(x = x_pred))

# Create scatter plot of the data
plot(x, y, pch = 16, main = "Quadratic Regression", xlab = "x", ylab = "y")

# Add the quadratic regression line
lines(x_pred, y_pred, col = "red", lwd = 2)
```
